# The search for "marihuana" in the website "www.duraznodigital.uy" brings up more than two pages
# The pattern between each URL of pages is that one of its numbers adds 20 each time it goes to a next page
# This happens because the page shows 20 news at a time
# After arriving to "120" the site has no more news to show
# Enter into ChatGPT the following prompt:
Build a scraper in R Studio that brings back the URL after "<a class='skin-color-hover' href=".
The data found should go into a new column called "MarihuanaDurazno"
The scraper should perform on this websites:
https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=120&by-date=false
https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=100&by-date=false
https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=80&by-date=false
https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=60&by-date=false
https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=40&by-date=false
https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=20&by-date=false
https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=0&by-date=false

# Paste the code into R Studio

# Load required libraries
library(rvest)
library(dplyr)
library(writexl)

# Function to scrape URLs from the given URL
get_urls_from_page <- function(url) {
  page <- read_html(url)
  urls <- page %>% html_nodes("a.skin-color-hover") %>% html_attr("href")
  return(urls)
}

# Function to fetch URLs from all pages
fetch_all_urls <- function(urls) {
  all_urls <- character()
  for (url in urls) {
    page_urls <- get_urls_from_page(url)
    all_urls <- c(all_urls, page_urls)
  }
  return(all_urls)
}

# List of URLs to scrape
urls_list <- c(
  "https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=120&by-date=false",
  "https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=100&by-date=false",
  "https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=80&by-date=false",
  "https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=60&by-date=false",
  "https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=40&by-date=false",
  "https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=20&by-date=false",
  "https://www.duraznodigital.uy/search?q=marihuana&updated-max=2022-07-15T20:13:00-07:00&max-results=20&start=0&by-date=false"
)

# Fetch all URLs from the provided list of URLs
all_urls <- fetch_all_urls(urls_list)

# Create a data frame with the URLs
url_df <- data.frame(MarihuanaDurazno = all_urls)

# Print the data frame
print(url_df)

# Write the data frame to an .xlsx file
write_xlsx(url_df, path = "marihuana_urls.xlsx")


# Open the dataset in Google Sheets
# In a new column write the xPath "//title" and drag it down so that each URL has a "//title" next to it
# In another column write the formula "=IMPORTXML(cell with URL, cell with "//title")
# Drag that cell down, so that each URL gets its own formula
# The result should be the title of each news
# Repeat the last step with the xPath "//i[contains(@class, 'far fa-clock')]"
# This xPath should show the date of the news
